From: John Ogness <john.ogness@linutronix.de>
Date: Fri, 22 Sep 2023 17:35:04 +0000
Subject: [PATCH 107/108] printk: Add kthread for all legacy consoles
Origin: https://www.kernel.org/pub/linux/kernel/projects/rt/6.6/older/patches-6.6-rt12.tar.xz

The write callback of legacy consoles make use of spinlocks. This is
not permitted with PREEMPT_RT in many contexts.

Create a new kthread to handling printing of all the legacy consoles
(and nbcon consoles if boot consoles are registered).

Since the consoles are printing in a task context, it is no longer
appropriate to support the legacy handover mechanism.

These changes exist only for CONFIG_PREEMPT_RT.

Signed-off-by: John Ogness <john.ogness@linutronix.de>
Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
---
 kernel/printk/internal.h |    1 
 kernel/printk/nbcon.c    |   20 +++++
 kernel/printk/printk.c   |  176 ++++++++++++++++++++++++++++++++++++++---------
 3 files changed, 166 insertions(+), 31 deletions(-)

--- a/kernel/printk/internal.h
+++ b/kernel/printk/internal.h
@@ -91,6 +91,7 @@ void nbcon_free(struct console *con);
 bool nbcon_console_emit_next_record(struct console *con);
 void nbcon_kthread_create(struct console *con);
 void nbcon_wake_threads(void);
+void nbcon_legacy_kthread_create(void);
 
 /*
  * Check if the given console is currently capable and allowed to print
--- a/kernel/printk/nbcon.c
+++ b/kernel/printk/nbcon.c
@@ -1236,8 +1236,21 @@ static bool nbcon_atomic_emit_one(struct
  */
 bool nbcon_console_emit_next_record(struct console *con)
 {
+	struct uart_port *port = con->uart_port(con);
+	static DEFINE_SPINLOCK(shared_spinlock);
 	struct nbcon_cpu_state *cpu_state;
 	bool progress = false;
+	unsigned long flags;
+
+	/*
+	 * If there is no port lock available, fallback to a shared
+	 * spinlock. This serves to provide the necessary type of
+	 * migration/preemption disabling while printing.
+	 */
+	if (port)
+		spin_lock_irqsave(&port->lock, flags);
+	else
+		spin_lock_irqsave(&shared_spinlock, flags);
 
 	cpu_state = nbcon_get_cpu_state();
 
@@ -1258,6 +1271,11 @@ bool nbcon_console_emit_next_record(stru
 		progress = nbcon_atomic_emit_one(&wctxt);
 	}
 
+	if (port)
+		spin_unlock_irqrestore(&port->lock, flags);
+	else
+		spin_unlock_irqrestore(&shared_spinlock, flags);
+
 	return progress;
 }
 
@@ -1472,6 +1490,8 @@ static int __init printk_setup_threads(v
 	printk_threads_enabled = true;
 	for_each_console(con)
 		nbcon_kthread_create(con);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT) && serialized_printing)
+		nbcon_legacy_kthread_create();
 	console_list_unlock();
 	return 0;
 }
--- a/kernel/printk/printk.c
+++ b/kernel/printk/printk.c
@@ -460,6 +460,9 @@ bool have_boot_console;
 static int __read_mostly suppress_panic_printk;
 
 DECLARE_WAIT_QUEUE_HEAD(log_wait);
+
+static DECLARE_WAIT_QUEUE_HEAD(legacy_wait);
+
 /* All 3 protected by @syslog_lock. */
 /* the next printk record to read by syslog(READ) or /proc/kmsg */
 static u64 syslog_seq;
@@ -2278,8 +2281,8 @@ asmlinkage int vprintk_emit(int facility
 			    const struct dev_printk_info *dev_info,
 			    const char *fmt, va_list args)
 {
+	bool print_direct = serialized_printing && !IS_ENABLED(CONFIG_PREEMPT_RT);
 	int printed_len;
-	bool in_sched = false;
 
 	/* Suppress unimportant messages after panic happens */
 	if (unlikely(suppress_printk))
@@ -2291,7 +2294,8 @@ asmlinkage int vprintk_emit(int facility
 
 	if (level == LOGLEVEL_SCHED) {
 		level = LOGLEVEL_DEFAULT;
-		in_sched = true;
+		/* If called from the scheduler, we can not call up(). */
+		print_direct = false;
 	}
 
 	printk_delay(level);
@@ -2300,8 +2304,7 @@ asmlinkage int vprintk_emit(int facility
 
 	nbcon_wake_threads();
 
-	/* If called from the scheduler, we can not call up(). */
-	if (serialized_printing && !in_sched) {
+	if (print_direct) {
 		/*
 		 * The caller may be holding system-critical or
 		 * timing-sensitive locks. Disable preemption during
@@ -2319,12 +2322,10 @@ asmlinkage int vprintk_emit(int facility
 		if (console_trylock_spinning())
 			console_unlock();
 		preempt_enable();
-	}
 
-	if (in_sched)
+	} else {
 		defer_console_output();
-	else
-		wake_up_klogd();
+	}
 
 	return printed_len;
 }
@@ -2352,6 +2353,14 @@ EXPORT_SYMBOL(_printk);
 static bool pr_flush(int timeout_ms, bool reset_on_progress);
 static bool __pr_flush(struct console *con, int timeout_ms, bool reset_on_progress);
 
+static struct task_struct *nbcon_legacy_kthread;
+
+static inline void wake_up_legacy_kthread(void)
+{
+	if (nbcon_legacy_kthread)
+		wake_up_interruptible(&legacy_wait);
+}
+
 #else /* CONFIG_PRINTK */
 
 #define printk_time		false
@@ -2364,7 +2373,8 @@ static u64 syslog_seq;
 
 static bool pr_flush(int timeout_ms, bool reset_on_progress) { return true; }
 static bool __pr_flush(struct console *con, int timeout_ms, bool reset_on_progress) { return true; }
-
+static inline void nbcon_legacy_kthread_create(void) { }
+static inline void wake_up_legacy_kthread(void) { }
 #endif /* CONFIG_PRINTK */
 
 #ifdef CONFIG_EARLY_PRINTK
@@ -2603,6 +2613,8 @@ void resume_console(void)
 	}
 	console_srcu_read_unlock(cookie);
 
+	wake_up_legacy_kthread();
+
 	pr_flush(1000, true);
 }
 
@@ -2617,7 +2629,8 @@ void resume_console(void)
  */
 static int console_cpu_notify(unsigned int cpu)
 {
-	if (!cpuhp_tasks_frozen && serialized_printing) {
+	if (!cpuhp_tasks_frozen && serialized_printing &&
+	    !IS_ENABLED(CONFIG_PREEMPT_RT)) {
 		/* If trylock fails, someone else is doing the printing */
 		if (console_trylock())
 			console_unlock();
@@ -2875,31 +2888,38 @@ static bool console_emit_next_record(str
 		con->dropped = 0;
 	}
 
-	/*
-	 * While actively printing out messages, if another printk()
-	 * were to occur on another CPU, it may wait for this one to
-	 * finish. This task can not be preempted if there is a
-	 * waiter waiting to take over.
-	 *
-	 * Interrupts are disabled because the hand over to a waiter
-	 * must not be interrupted until the hand over is completed
-	 * (@console_waiter is cleared).
-	 */
-	printk_safe_enter_irqsave(flags);
-	console_lock_spinning_enable();
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		/*
+		 * While actively printing out messages, if another printk()
+		 * were to occur on another CPU, it may wait for this one to
+		 * finish. This task can not be preempted if there is a
+		 * waiter waiting to take over.
+		 *
+		 * Interrupts are disabled because the hand over to a waiter
+		 * must not be interrupted until the hand over is completed
+		 * (@console_waiter is cleared).
+		 */
+		printk_safe_enter_irqsave(flags);
+		console_lock_spinning_enable();
 
-	/* Do not trace print latency. */
-	stop_critical_timings();
+		/* Do not trace print latency. */
+		stop_critical_timings();
+	}
 
 	/* Write everything out to the hardware. */
 	con->write(con, outbuf, pmsg.outbuf_len);
 
-	start_critical_timings();
-
 	con->seq = pmsg.seq + 1;
 
-	*handover = console_lock_spinning_disable_and_check(cookie);
-	printk_safe_exit_irqrestore(flags);
+	if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
+		*handover = false;
+	} else {
+		start_critical_timings();
+
+		*handover = console_lock_spinning_disable_and_check(cookie);
+
+		printk_safe_exit_irqrestore(flags);
+	}
 skip:
 	return true;
 }
@@ -3289,11 +3309,93 @@ void console_start(struct console *conso
 
 	if (flags & CON_NBCON)
 		nbcon_kthread_wake(console);
+	else
+		wake_up_legacy_kthread();
 
 	__pr_flush(console, 1000, true);
 }
 EXPORT_SYMBOL(console_start);
 
+#ifdef CONFIG_PRINTK
+static bool printer_should_wake(u64 seq)
+{
+	bool available = false;
+	struct console *con;
+	int cookie;
+
+	if (kthread_should_stop())
+		return true;
+
+	cookie = console_srcu_read_lock();
+	for_each_console_srcu(con) {
+		short flags = console_srcu_read_flags(con);
+
+		if (flags & CON_NBCON)
+			continue;
+		if (!console_is_usable(con, flags, true))
+			continue;
+		/*
+		 * It is safe to read @seq because only this
+		 * thread context updates @seq.
+		 */
+		if (prb_read_valid(prb, con->seq, NULL)) {
+			available = true;
+			break;
+		}
+	}
+	console_srcu_read_unlock(cookie);
+
+	return available;
+}
+
+static int nbcon_legacy_kthread_func(void *unused)
+{
+	u64 seq = 0;
+	int error;
+
+	for (;;) {
+		error = wait_event_interruptible(legacy_wait, printer_should_wake(seq));
+
+		if (kthread_should_stop())
+			break;
+
+		if (error)
+			continue;
+
+		console_lock();
+		seq = console_flush_and_unlock();
+	}
+	return 0;
+}
+
+void nbcon_legacy_kthread_create(void)
+{
+	struct task_struct *kt;
+
+	lockdep_assert_held(&console_mutex);
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT))
+		return;
+
+	if (!printk_threads_enabled || nbcon_legacy_kthread)
+		return;
+
+	kt = kthread_run(nbcon_legacy_kthread_func, NULL, "pr/legacy");
+	if (IS_ERR(kt)) {
+		pr_err("unable to start legacy printing thread\n");
+		return;
+	}
+
+	nbcon_legacy_kthread = kt;
+
+	/*
+	 * It is important that console printing threads are scheduled
+	 * shortly after a printk call and with generous runtime budgets.
+	 */
+	sched_set_normal(nbcon_legacy_kthread, -20);
+}
+#endif /* CONFIG_PRINTK */
+
 static int __read_mostly keep_bootcon;
 
 static int __init keep_bootcon_setup(char *str)
@@ -3550,6 +3652,7 @@ void register_console(struct console *ne
 		nbcon_init(newcon);
 	} else {
 		have_legacy_console = true;
+		nbcon_legacy_kthread_create();
 	}
 
 	if (newcon->flags & CON_BOOT)
@@ -3686,6 +3789,13 @@ static int unregister_console_locked(str
 			nbcon_kthread_create(c);
 	}
 
+#ifdef CONFIG_PRINTK
+	if (!serialized_printing && nbcon_legacy_kthread) {
+		kthread_stop(nbcon_legacy_kthread);
+		nbcon_legacy_kthread = NULL;
+	}
+#endif
+
 	return res;
 }
 
@@ -3844,8 +3954,12 @@ static bool __pr_flush(struct console *c
 
 	seq = prb_next_seq(prb);
 
-	/* Flush the consoles so that records up to @seq are printed. */
-	if (serialized_printing) {
+	/*
+	 * Flush the consoles so that records up to @seq are printed.
+	 * Otherwise this function will just wait for the threaded printers
+	 * to print up to @seq.
+	 */
+	if (serialized_printing && !IS_ENABLED(CONFIG_PREEMPT_RT)) {
 		console_lock();
 		console_unlock();
 	}
@@ -3950,7 +4064,7 @@ static void wake_up_klogd_work_func(stru
 
 	if (pending & PRINTK_PENDING_OUTPUT) {
 		if (IS_ENABLED(CONFIG_PREEMPT_RT)) {
-			;
+			wake_up_interruptible(&legacy_wait);
 		} else {
 			/*
 			 * If trylock fails, some other context
